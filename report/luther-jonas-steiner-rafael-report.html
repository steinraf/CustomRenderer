<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Computer Graphics - Final Report</title>

    <link href="resources/bootstrap.min.css" rel="stylesheet">
    <link href="resources/offcanvas.css" rel="stylesheet">
    <link href="resources/custom2014.css" rel="stylesheet">
    <link href="resources/twentytwenty.css" rel="stylesheet" type="text/css" />
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

<div class="container headerBar">
		<h1>Final Project Report - Jonas Luther and Rafael Steiner</h1>
</div>

<div class="container contentWrapper">
<div class="pageContent">

	<!-- ================================================================= -->

    <p>
        This report is only for the features of Rafael Steiner.
    </p>

    <h2>Feature 1: Simple Denoising + Adaptive Sampling</h2>

    <h3> Simple Denoising: Bilateral Filter with Pixel Variance Estimate</h3>

    <p>
        Relevant files: <br>
        <code>src/denoise/denoise.h</code> <br>
        <code>src/denoise/denoise.cu</code> <br>
        <code>src/cudaHelpers.h</code> <br>

        During rendering, a feature buffer is built that contains the color variance,
        the albedo of the first hit, the spatial position of the first hit, the normal of the first hit
        and the total number of subSamples in a given pixel. For the current bilater filter denoiser,
        only the variance of the color is used, but the other buffers were created for extensibility of the denoiser
        and debugging purposes.

        The denoiser itself is a bilateral filter with window size 11 and k = 0.45.

        To not have to store an array for every entry, the variance is computed incrementally
        using Welford's online algorithm.

    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox64spp.png" alt="Original Render" class="img-responsive">
        <img src="images/cbox64sppVariance.png" alt="Variance Buffer" class="img-responsive">
        <img src="images/cbox64sppDenoised.png" alt="Denoised Render" class="img-responsive">
    </div> <br>

    <p>
        The quality of the denoiser was tested in this cornell box scene with 64 samples per pixel.
        The denoiser is very effective at removing fireflies as their variance is large. For low sample counts,
        it tends to remove caustics as well, but this is to be expected as their variance is also high.
    </p>

    <h3> Adaptive Sampling with Pixel Variance Estimates </h3>

    <p>
        Relevant files: <br>
        <code>src/cudaHelpers.h</code> <br>

        The adaptive sampling takes into consideration the previously computed variance buffer.
        To make it more robust, the variances are considered in a 4x4 block size, which is conveniently the kernel size of the
        GPU call. If all variances in the square are below some threshold Epsilon, the grid is no longer sampled.
        <br>


        Because this implementation does not distribute samples globally, it does not fully make use of all
        spp * width * height samples that it is given. This can be offset by adjusting the spp higher if the scene
        has areas of low variance.
    </p>

    <div class="twentytwenty-container">
        <img src="images/adaptiveSampling.png" alt="Output" class="img-responsive">
        <img src="images/adaptiveSamplingSamples.png" alt="Sample Distribution" class="img-responsive">
    </div> <br>




    <h2>Feature 2: Advanced Camera Model</h2>

    <p>
        Relevant files: <br>
        <code>src/camera/camera.h</code> <br>
        <code>src/camera/camera.cu</code> <br>

        For this task, a thinlens camera model was implemented. It supports different field of view angles,
        aspect ratios, aperture radii and shapes, focal lengths and two distortion parameters.
    </p>

    <img src="images/cbox512sppCameraTest.png" alt="Reference" class="img-responsive">

    <p>
        To test the camera a scene inspired by the cornell box was created.
        <br>
        The scene contains various small square light sources arranged in a circular shape, all facing towards the positive z-direction.
    </p>

    <h3>Depth of Field</h3>

    <p>
        The depth of field makes object in the focal plane look focused, while objects outside the focal plane become blurred.
        The DoF does not add any distortion to the image.
    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDepthOfField.png" alt="Own" class="img-responsive">
        <img src="images/cbox512sppDepthOfFieldMitsuba.png" alt="Mitsuba Reference" class="img-responsive">
    </div> <br>

    <p>
        The focal length was chosen such that some lights are in front of the focal plane, while some are behind.

        The render was compared to Mitsuba 3.0. It can be seen that there is a difference in the renders. From debugging it
        seems that this has to do with reconstruction filters, which this framework does not support (yet).
    </p>

    <h3>Lens Distortion</h3>

    <p>
        The Lens Distortion model used in the renderer is pretty simple. It contains two distortion term.
        K1 distorts the image based on the square of the radial distance of the camera plane, while the parameter K2 distorts
        the image proportional to the fourth power of the distance. This models spherical distortion and is inspired from

        <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">this paper</a>.

    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDistortionK1.png" alt="K1 Distortion " class="img-responsive">
        <img src="images/cbox512sppDistortionK1K2.png" alt="K1 + K2 Distortion" class="img-responsive">
        <img src="images/cbox512sppCameraTest.png" alt="Undistorted" class="img-responsive">

    </div> <br>

    <h3>Non-Spherical Aperture</h3>

    <p>
        The aperture can have three different shapes; (The default) circular aperture, a square aperture and a triangular aperture.
        All the apertures are centered and their size depends on the aperture radius.
    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDepthOfField.png" alt="Circular Aperture" class="img-responsive">
        <img src="images/cbox512sppSquare.png" alt="Square Aperture" class="img-responsive">
        <img src="images/cbox512sppTriangle.png" alt="Triangular Aperture" class="img-responsive">
    </div> <br>



    <h2>Feature 3: Environment Map Emitters</h2>

    <p>
        Relevant files: <br>
        <code>src/textures/imageTexture.h</code> <br>
        <code>src/textures/imageTexture.cu</code> <br>
        <code>src/emitters/environmentEmitter.h</code> <br>

    </p>

	<p>Environment Map Emitters are implemented using Latitude-Longitude maps. The map is converted into 1-D CDF,
        from which a point is then sampled. This approach struggles with images with very high average intensity, as the
        normalization divides a lot of small values to zero.
    <br>
    </p>

    <div class="twentytwenty-container">
        <img src="images/envmapCDF.png" alt="Rowwise CDF" class="img-responsive">
        <img src="images/envmapSampling.png" alt="Sampling Density" class="img-responsive">
        <img src="images/beach.png" alt="Original Environment Map" class="img-responsive">
    </div> <br>

    <p>
        The environment map importance sampling works as can be seen by the diffusive sphere in the examle render.
        The discrete BSDF's work well without it.
    </p>

    <div class="twentytwenty-container">
        <img src="images/envmap512spp.png" alt="Example Render" class="img-responsive">
    </div> <br>


    <h2>Feature 4: Images as Textures</h2>

    <p>
        Relevant files: <br>
        <code>src/textures/imageTexture.h</code> <br>
        <code>src/textures/imageTexture.cu</code> <br>

        To make texture support work, textures are loaded into CPU memory with help of the stbimage library.
        The texture then gets transformed into a CDF, which makes it possible to importance sample it according to texture.

        The textures are evaluated using bilinear interpolation to make artifacts a bit less visible.
    </p>

    <div class="twentytwenty-container">
        <img src="images/spot.png" alt="Own" class="img-responsive">
        <img src="images/spotMitsuba.png" alt="Mitsuba" class="img-responsive">
    </div> <br>

    <p>

        The render matches with mitsuba up to the camera difference.
    </p>


    <h2>Feature 5: Normal Mapping</h2>

    <p>

        Relevant files: <br>
        <code>src/shapes/triangle.h</code> <br>
        <code>src/textures/imageTexture.h</code> <br>
        <code>src/textures/imageTexture.cu</code> <br>

        To make normal mapping work, normal maps are mapped onto the Mesh, and the hit information functions
    is updated according to the normal map. </p>

    <div class="twentytwenty-container">
        <img src="images/rockTexture.png" alt="Regular Texture" class="img-responsive">
        <img src="images/rockNormal.png" alt="Normal Mapped" class="img-responsive">
        <img src="images/rockTextureMitsuba.png" alt="Regular Texture Mitsuba" class="img-responsive">
        <img src="images/rockNormalMitsuba.png" alt="Normal Mapped Mitsuba" class="img-responsive">
    </div> <br>

    <p>While the texture matches, the normal maps dont seem to match mitsuba, but they look good on their own.</p>


    <h2>Feature 6: Textured Area Emitters</h2>

    <p>To make textured area emitters work, the texture is projected onto the uv coordinates of a mesh and multiplied
    by the radiance of the emitter.</p>

    <div class="twentytwenty-container">
        <img src="images/texturedEmitter.png" alt="Rowwise CDF" class="img-responsive">
    </div> <br>


    <h1>
        Addendum
    </h1>

    <h2>
        Cuda Path Tracing
    </h2>

    <p>
        My work is written using a renderer that I made specifically for the computer graphics course.
        The renderer is written using CUDA and runs fully on the GPU.

        Because writing a whole renderer brings quite some challenges with itself, some features were omitted or simplified.

        Here I will talk a bit about all the features that were implemented.


    </p>

    <h3>
        Acceleration Structures
    </h3>
        <p>
            <code>src/acceleration</code> <br>

            To make the rendering feasible, an acceleration is needed for raytracing. I chose a Bounding Volume Hierarchy filled with Axis Aligned Bounding Boxes.
            After the xml is read on the CPU, the meshes are transferred onto the gpu, and each mesh gets its own BVH. The idea
            behind this was that a Bottom Layer Acceleration Structure would hold all the Meshes, and Top Layer Acceleration Structure would be a BVH that contains meshes.
            Due to time constraints, only the Bottom Layer Acceleration offers logarithmic traversal.

            Furthermore, the only optimization for the BVH is that the triangles get sorted according to morton code, which
            provides some spacial locality. Other optimizations like SAH optimizations were explored, but didnt make it in the final version.



        </p>

    <h3>
        Camera
    </h3>

    <p>
        <code>src/acceleration</code> <br>
        The camera pretty much works like in nori, but it has the handedness like mitsuba or other renderers.
        There is a slight offset in the camera that makes renderer noticably offset if inspected closely, but overall it is equivalent.
    </p>

    <h3>
        Denoiser
    </h3>
    <p>
        <code>src/denoise</code> <br>

        One thing that differs from the base Nori framework, is that there are no reconstruction filter.
        This introduces some noise/unwanted artifacts.
    </p>

    <h3>
        Emitters
    </h3>
    <p>
        <code>src/areaLight</code> <br>
        For simplicity and performance (virtual function overhead), there is only one kind of emitter, namely the mesh area emitter.
        It supports sampling by surface area, which is achieved using a CDF over all triangle areas for a given BVH.

    </p>

    <h3>
        Media
    </h3>
    <p>
        <code>src/medium</code> <br>
        Some media were ported onto the GPU, which do work for some simplified scenes.
        Due to timeconstraints they were not fully ported.
    </p>

    <h3>
        Scene
    </h3>
    <p>
        <code>src/scene</code> <br>
        The scene framework was meant to use OpenGL to procedurally show the renderer, but this feature did not make it in time.
        To estimate progress,  a counter keeps track of the progress made.

        The project features an xml parser, which parses xml's with a mitsuba inspired syntax.
    </p>

    <h3>
        Primitives
    </h3>
    <p>
        <code>src/shapes</code> <br>
        For similar reasons as the area emitters, only one type of primitive, namely the triangle is supported.
        As meshes are powerful, this was a trade off that was well worth it.
    </p>

    <h3>
        Textures
    </h3>
    <p>
        <code>src/textures</code> <br>

        As textures are again not virtual classes, there is some overhead for each texture, but this is
        not relevant in practice.
        The possibility of hardware accelerated GPU texture lookups was investigated, but didnt make it into the final version.


    </p>

    <h3>
        Utility
    </h3>
    <p>
        <code>src/utility</code> <br>

        These functions are often very much like the nori equivalents and are made with them in mind.
        The sampling is done on the gpu using curand.

    </p>

    <h3>
        BSDF's
    </h3>
    <p>
        <code>src/bsdf.h</code> <br>

        Again, BSDF's are not virtual functions, some trade offs needed to be made.
        The renderer support Diffuse, Dielectric and Mirror BSDF's, and is in principle extensible, but using enums is a bit tedius
        and unsafe.
    </p>

    <h3>
        Helper Functions
    </h3>
    <p>
        <code>src/cudaHelpers</code> <br>

        All the main CUDA kernels are stored here. This includes the main render loop and some
        GPU allocation function, as well as the integrators.

        The integrators are implemented as functions (or inside functions) and can be switched by commenting out the line (not all might work anymore).

    </p>




</div>
</div>


<!-- Bootstrap core JavaScript -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="resources/bootstrap.min.js"></script>
<script src="resources/jquery.event.move.js"></script>
<script src="resources/jquery.twentytwenty.js"></script>


<script>
$(window).load(function(){$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5});});
</script>

</body>
</html>
