<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Computer Graphics - PA4</title>

    <link href="resources/bootstrap.min.css" rel="stylesheet">
    <link href="resources/offcanvas.css" rel="stylesheet">
    <link href="resources/custom2014.css" rel="stylesheet">
    <link href="resources/twentytwenty.css" rel="stylesheet" type="text/css" />
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

<div class="container headerBar">
		<h1>Final Project Report - Jonas Luther and Rafael Steiner</h1>
</div>

<div class="container contentWrapper">
<div class="pageContent">

	<!-- ================================================================= -->

    <h2>Feature 1: Simple Denoising + Adaptive Sampling</h2>

    <h3> Simple Denoising: Bilateral Filter with Pixel Variance Estimate</h3>

    <p>
        Relevant files: <br>
        <code>src/denoise/denoise.h</code> <br>
        <code>src/denoise/denoise.cu</code> <br>
        <code>src/cudaHelpers.h</code> <br>

        During rendering, a feature buffer is built that contains the color variance,
        the albedo of the first hit, the spatial position of the first hit, the normal of the first hit
        and the total number of subSamples in a given pixel. For the current bilater filter denoiser,
        only the variance of the color is used, but the other buffers were created for extensibility of the denoiser
        and debugging purposes.

        The denoiser itself is a bilateral filter with window size 11 and k = 0.45.

        To not have to store an array for every entry, the variance is computed incrementally
        using Welford's online algorithm.

    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox64spp.png" alt="Original Render" class="img-responsive">
        <img src="images/cbox64sppVariance.png" alt="Variance Buffer" class="img-responsive">
        <img src="images/cbox64sppDenoised.png" alt="Denoised Render" class="img-responsive">
    </div> <br>

    <p>
        The quality of the denoiser was tested in this cornell box scene with 64 samples per pixel.
        The denoiser is very effective at removing fireflies as their variance is large. For low sample counts,
        it tends to remove caustics as well, but this is to be expected as their variance is also high.
    </p>

    <h3> Adaptive Sampling with Pixel Variance Estimates </h3>

    <p>
        Relevant files: <br>
        <code>src/cudaHelpers.h</code> <br>

        The adaptive sampling takes into consideration the previously computed variance buffer.
        To make it more robust, the variances are considered in a 4x4 block size, which is conveniently the kernel size of the
        GPU call. If all variances in the square are below some threshold Epsilon, the grid is no longer sampled.
        <br>


        Because this implementation does not distribute samples globally, it does not fully make use of all
        spp * width * height samples that it is given. This can be offset by adjusting the spp higher if the scene
        has areas of low variance.
    </p>

    <div class="twentytwenty-container">
        <img src="images/adaptiveSampling.png" alt="Output" class="img-responsive">
        <img src="images/adaptiveSamplingSamples.png" alt="Sample Distribution" class="img-responsive">
    </div> <br>




    <h2>Feature 2: Advanced Camera Model</h2>

    <p>
        Relevant files: <br>
        <code>src/camera/camera.h</code> <br>
        <code>src/camera/camera.cu</code> <br>

        For this task, a thinlens camera model was implemented. It supports different field of view angles,
        aspect ratios, aperture radii and shapes, focal lengths and two distortion parameters.
    </p>

    <img src="images/cbox512sppCameraTest.png" alt="Reference" class="img-responsive">

    <p>
        To test the camera a scene inspired by the cornell box was created.
        <br>
        The scene contains various small square light sources arranged in a circular shape, all facing towards the positive z-direction.
    </p>

    <h3>Depth of Field</h3>

    <p>
        The depth of field makes object in the focal plane look focused, while objects outside the focal plane become blurred.
        The DoF does not add any distortion to the image.
    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDepthOfField.png" alt="Own" class="img-responsive">
        <img src="images/cbox512sppDepthOfFieldMitsuba.png" alt="Mitsuba Reference" class="img-responsive">
    </div> <br>

    <p>
        The focal length was chosen such that some lights are in front of the focal plane, while some are behind.

        The render was compared to Mitsuba 3.0. It can be seen that there is a difference in the renders. From debugging it
        seems that this has to do with reconstruction filters, which this framework does not support (yet).
    </p>

    <h3>Lens Distortion</h3>

    <p>
        The Lens Distortion model used in the renderer is pretty simple. It contains two distortion term.
        K1 distorts the image based on the square of the radial distance of the camera plane, while the parameter K2 distorts
        the image proportional to the fourth power of the distance. This models spherical distortion and is inspired from

        <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">this paper</a>.

    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDistortionK1.png" alt="K1 Distortion " class="img-responsive">
        <img src="images/cbox512sppDistortionK1K2.png" alt="K1 + K2 Distortion" class="img-responsive">
        <img src="images/cbox512sppCameraTest.png" alt="Undistorted" class="img-responsive">

    </div> <br>

    <h3>Non-Spherical Aperture</h3>

    <p>
        The aperture can have three different shapes; (The default) circular aperture, a square aperture and a triangular aperture.
        All the apertures are centered and their size depends on the aperture radius.
    </p>

    <div class="twentytwenty-container">
        <img src="images/cbox512sppDepthOfField.png" alt="Circular Aperture" class="img-responsive">
        <img src="images/cbox512sppSquare.png" alt="Square Aperture" class="img-responsive">
        <img src="images/cbox512sppTriangle.png" alt="Triangular Aperture" class="img-responsive">
    </div> <br>



    <h2>Feature 3: Environment Map Emitters</h2>

	<p>Environment Map Emitters are implemented using Latitude-Longitude maps. The map is converted into 1-D CDF,
        from which a point is then sampled. This approach struggles with images with very high average intensity, as the
        normalization divides a lot of small values to zero.
    <br>


    </p>

    <div class="twentytwenty-container">
        <img src="images/envmapCDF.png" alt="Rowwise CDF" class="img-responsive">
        <img src="images/envmapSampling.png" alt="Sampling Density" class="img-responsive">
        <img src="images/beach.png" alt="Original Environment Map" class="img-responsive">
    </div> <br>

    <div class="twentytwenty-container">
        <img src="images/envmap512spp.png" alt="Example Render" class="img-responsive">
    </div> <br>


    <h2>Feature 4: Images as Textures</h2>

    <p>
        Relevant files: <br>
        <code>src/textures/imageTexture.h</code> <br>
        <code>src/textures/imageTexture.cu</code> <br>

        To make texture support work, textures are loaded into CPU memory with help of the stbimage library.
        The texture then gets transformed into a CDF, which makes it possible to importance sample it according to texture.

        The textures are evaluated using bilinear interpolation to make artifacts a bit less visible.
    </p>

    <div class="twentytwenty-container">
        <img src="images/spot.png" alt="Own" class="img-responsive">
        <img src="images/spotMitsuba.png" alt="Mitsuba" class="img-responsive">
    </div> <br>

    <p>
        The render matches with mitsuba up to the camera difference.
    </p>


    <h2>Feature 5: Normal Mapping</h2>

    <p>Copium</p>

    <div class="twentytwenty-container">
        <img src="images/rockTexture.png" alt="Regular" class="img-responsive">
        <img src="images/rockNormal.png" alt="Normal Mapped" class="img-responsive">
        <img src="images/rockTextureMitsuba.png" alt="Regular Mitsuba" class="img-responsive">
        <img src="images/rockNormalMitsuba.png" alt="Normal Mapped Mitsuba" class="img-responsive">
    </div> <br>


    <h2>Feature 6: Textured Area Emitters</h2>

    <p>TODO FIX</p>

    <div class="twentytwenty-container">
        <img src="images/texturedEmitter.png" alt="Rowwise CDF" class="img-responsive">
        <img src="images/envmapSampling.png" alt="Sampling Density" class="img-responsive">
    </div> <br>





</div>
</div>


<!-- Bootstrap core JavaScript -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="resources/bootstrap.min.js"></script>
<script src="resources/jquery.event.move.js"></script>
<script src="resources/jquery.twentytwenty.js"></script>


<script>
$(window).load(function(){$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5});});
</script>

</body>
</html>
